\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\begin{document}

\title{Reversing Conway's Game of Life\\
{\footnotesize CS229 Fall 2020 Project Proposal \hspace{2cm}
Project category: General Machine Learning}
}



%\icmlcorrespondingauthor{}{johnjia@stanford.edu}
%\icmlcorrespondingauthor{Sebastian Kochman}{sebastko@stanford.edu}
%\icmlcorrespondingauthor{Jianyu Lu}{jylux@stanford.edu}
\author{\IEEEauthorblockN{Johnson Xin Jia}
\IEEEauthorblockA{johnjia@stanford.edu}
\and
\IEEEauthorblockN{Sebastian Kochman}
\IEEEauthorblockA{sebastko@stanford.edu}
\and
\IEEEauthorblockN{Jianyu Lu}
\IEEEauthorblockA{jylux@stanford.edu}
}

\maketitle

%\begin{abstract}
%The Game of Life is a cellular automaton created by the mathematician John Conway. The game consists of a board of cells that are either live or dead. One creates an initial configuration of these live/dead states and observes how it evolves \cite{b1} \cite{b2} following deterministic rules. The goal of this project is to use machine learning to reverse the game by finding one of the potentially many possible preceding states from a given final state.
%\end{abstract}

\begin{comment}
\begin{IEEEkeywords}
convolutional neural networks, Kaggle competition, game, simulators
\end{IEEEkeywords}
\end{comment}

\section{Motivation}
The idea for this project came from a Kaggle competition \cite{b1} where we are to predict the {\it starting state} of 25x25 board given the state of the board after a number of evolutions (between 1 to 5) following the rules prescribed in Conway's Game of Life. The data to learn from consists of the {\it terminal states}, the corresponding starting states (which are the labels), and the number of evolutions applied to go from the latter to the former. Our motivation to take on this project stems from some unique challenges and advantages it presents. In terms of challenges, this problem of reversing Conway's Game of Life does not fit into the mold of a standard regression or classification problem where one can immediately fit a model; instead, some ingenuity is needed to frame the problem into one amendable to ML techniques. In terms of advantages, the setting of this problem is in an idealized world where we have access to the data generator; so the key to learning a good model relies less on data processing, cleaning, feature engineering, hyperparameter tuning, but instead depends more on a deep understand of the problem and creative application of the modeling techniques that can model the relationship between the starting states and the terminal states. 

\section{Method}

Firstly, we would like to note that solving the problem for a single step backwards should provide a solution for $N$ steps backwards, by repeating the process.

Secondly, we can use the fact that it is easy to simulate the Game of Life forward - hence, it should be easy to generate new training examples as well as validate model's predictions at any time.

Using these facts, we can divide methods to explore along three dimensions:

\begin{enumerate}
    \item Model architectures used to learning representions of the board state -- we plan to experiment with fully-connected neural networks, convolutional neural networks, and graph convolutional neural networks. We are considering techniques to combat the problem of high sample complexity by sharing weights between layers or dividing input and/or output into smaller chunks (e.g. 3x3).
    
    \item Learning paradigm -- supervised learning using examples generated by a simulator will be our primary machine learning paradigm. We also plan to explore some more speculative ideas, such as variational autoencoder, reinforcement learning, and generative adversarial networks (GANs).
    
    \item Additional tricks leveraging search/heuristics -- since we have access to a simulator which can be used to validate output of the ML model at each step, we can use that fact to correct the model's errors by searching nearby states, e.g., inference code could examine and validate not only top 1, but top $K$ highest scored bitmaps according to the ML model. 
    
    % While designing any such strategy, we must keep in mind the code run time limit set by the Kaggle competition (2 hours on GPU or 9 hours on CPU for processing 50k test examples).
    
\end{enumerate}

\section{Intended Experiments}

As outlined the previous sections, our initial experiments are supervised learning algorithms using either a multi-layer perceptron or a CNN (or possible a GCN) to predict the starting state pixel-by-pixel. We also plan to perform experiments using generative models (something along the lines of learning a joint probability distribution $\mathbf P(x, y)$ where $x, y$ are pixels/regions in the starting and terminal state respectively). We also have some other ideas around RL and GAN that we may explore time permitting. As we are a team of three, we plan to each take on an idea to experiment with for the first two weeks and settle on the one that is most promising once we have some results.

For model evaluation, we will use the evaluation criterion defined in the Kaggle competition, which is the mean absolute error of the terminal state reach by our model's prediction and the actual terminal state. Mathematically, this is given by $$\frac{\sum_{i=1}^{m}\sum_{j=1}^{n}|y_{j}^{(i)} - \hat y_{j}^{(i)}|}{m \times n},$$ where $y_{j}^{(i)}$ is the value of the $j$th cell in the $i$th ground truth terminal state, and $\hat y_{j}^{(i)}$ is the value of the $j$th cell from the $i$th predicted starting board after evolution.


\begin{thebibliography}{00}
\bibitem{b1} ``Conway's Reverse Game of Life 2020,'' \url{https://www.kaggle.com/c/conways-reverse-game-of-life-2020/overview}
\bibitem{b2} ``Conway's Game of Life,'' Wikipedia \url{https://en.wikipedia.org/wiki/Conway\%27s_Game_of_Life}
\end{thebibliography}

\end{document}
