\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage[ruled]{algorithm2e}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\newcommand\todo[1]{\textcolor{red}{#1}}

\renewcommand{\thesection}{\arabic{section}}
\def\thesectiondis{\thesection.} \def\thesubsectiondis{\thesectiondis\arabic{subsection}.} \def\thesubsubsectiondis{\thesubsectiondis\arabic{subsubsection}.} \def\theparagraphdis{\thesubsubsectiondis\arabic{paragraph}.}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\begin{document}

\title{Reversing Conway's Game of Life}

%\icmlcorrespondingauthor{}{johnjia@stanford.edu}
%\icmlcorrespondingauthor{Sebastian Kochman}{sebastko@stanford.edu}
%\icmlcorrespondingauthor{Jianyu Lu}{jylux@stanford.edu}
\author{\IEEEauthorblockN{Johnson Jia}
\IEEEauthorblockA{johnjia@stanford.edu}
\and
\IEEEauthorblockN{Sebastian Kochman}
\IEEEauthorblockA{sebastko@stanford.edu}
\and
\IEEEauthorblockN{Jianyu Lu}
\IEEEauthorblockA{jylux@stanford.edu}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
The Game of Life \cite{wiki:GameOfLife} is a cellular automaton created by the mathematician John Conway. It consists of a board of cells that are either live or dead, such that each cell evolves according to a set of deterministic rules  based on the state of its neighboring cells. We apply machine learning to reverse the Game of Life, that is, to generate one of the many preceding states that evolve to a given state. The essence of the problem is approximating an inverse of a complex non-linear function which has many potential applications in the real world.\footnote{Code is available at \url{https://github.com/sebastiankochman/jjs229}}
\end{abstract}

\section{Introduction}
\label{intro}
The Game of Life is a \emph{life-like} cellular automaton \cite{wiki:LifeLikeAutomaton} where a two-dimensional board of live or dead cells evolve according to the following two simple rules:
\begin{enumerate}
    \item A live cell that is surrounded by two or three lives cells survives in the next round, otherwise it dies.
    \item A dead cell that is surrounded by three living cells comes alive the next round, otherwise it stays dead.
\end{enumerate}
Whereas this forward evolution in the Game of Life can be modeled perfectly using a convolutional neural network---in fact, one such CNN is prescribed in the appendix of  \cite{springer2020its}---learning to reverse the Game of Life is much more challenging. For one, many different boards can evolve to the same board, so a supervised learning approach with the evolved boards as the input and their predecessors---that is, the boards from which they evolved---as the output is unlikely to work. In this paper, we describe an approach which partially overcomes this one-to-many problem when reversing the Game of Life.

As the idea for this project came from a Kaggle competition \cite{Kaggle}, we adopt many of the problem settings outlined in the competition (with some minor differences).\footnote{In particular, the competition provides a public test set which can be solved using expensive search-based methods offline, while we focus on developing a machine learning model that would generalize to unseen cases. Also, the competition requires reversing multiple steps whereas we solely focus on reversing just one step in this paper.} \label{wraparoundboard} We will focus on boards of size $25 \times 25$, with wrap-around in the sense that cells on the left-edge (top-edge) are considered adjacent to the cells on the right-edge (bottom-edge) in the same row (column). We also focus on reversing just one step of the Game of Life, as its difficulty is representative of the general problem of reversing $\delta$ steps, and a model that does well in this special case should in theory be easy to generalize to the general case.\footnote{Although in actuality we noticed that minor errors in the one-step reversal get amplified during repeated application of the model to reverse multiple steps. We leave this generalization work for the future.}

\subsection{Notation}
In order to aid the discussion, we adopt the following notation:

\begin{itemize}
    \item $s, e \in \{0, 1\}^{25\times 25}$: a pair of matrices with entries in $0$ (dead or off) or $1$ (live or on) representing an example of consecutive states (or ``boards'') in the Game of Life: $s$ denotes the starting board and $e$ denotes a corresponding one-step evolution, which we refer to as the \emph{evolved} or stop board\footnote{Even though $s$ and $e$ are matrices, we opted for the lower-case notation to emphasize these are individual examples in a large data set.}.
    \item $s^{(i)}, e^{(i)}$: superscript $(i)$ denotes an $i$th example of evolution in some data set.
    \item $s_{j,k}$ or $e_{j,k}$: the value of the cell in the $j$th row and $k$th column of $s$ or $e$ correspondingly. We consider the boards to be wrapped around the edges, so each cell has exactly 8 neighbors (or adjacent cells).
    \item Where applicable we'll use the hat symbol (e.g. $\hat s$) to denote predicted outputs.
    \item The perfect forward simulator of the Game of Life implementing the rules described in Section \ref{intro} is denoted as function $\pi$, so $e^{(i)} = \pi(s^{(i)})$.
\end{itemize}

Using this notation, our goal is to train a model that, given an evolved board $e^{(i)}$, predicts one of the (potentially) many $s^{(i)}$'s that evolve to a given $e^{(i)}$.

\section{Related Work}
In \cite{springer2020its}, the authors worked on training a neural network to predict the forward evolution. This is a simpler problem with a known solution---a three-layered CNN---alluded to earlier. The authors however showed that training such a three-layered CNN from scratch fails to learn the forward evolution, but instead one has to increase the size of the model substantially in order to learn it with perfect accuracy. Similarly, Mordvintsev et al. also use machine learning to evolve cellular automaton in \cite{mordvintsev2020growing} from the perspective of studying differentiable self-organizing systems.

In principle, the problem we study is finding an inverse of a complex non-linear function. Independent component analysis (ICA)  is an unsupervised method of finding an inverse of a linear transformation that produced observed data. There has been recently a lot of progress in studying non-linear ICA \cite{hyvarinen2019nonlinear} \cite{brakel2017learning}, which often involve Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}. While the specific problems studied in these works are not directly related to the Game of Life, there is still a tenuous connection in the underlying techniques. In particular, we found some amount of success in applying a technique similar to GAN to reverse the Game of Life.

\section{Dataset}
\label{dataset}
We created a data generator (adapting the procedure prescribed by the aforementioned Kaggle competition to the case of one-step evolution) to generate the datasets used for training and evaluation.

\begin{enumerate}
\item An initial board is chosen by filling the board with a uniformly random density of live cells between 1\% (mostly 0's) and 99\% (mostly 1's).
\item This initial board is evolved using $\pi$ by 5 to 9 steps to ``warm up'' the board. This is partly to evolve the board into a state that's more ``life-like''. This post-warm-up board will be a starting board $s$.
\item We then evolve $s$ one step to get $e = \pi(s)$. If $e$ turns out to be empty (a matrix containing only zeros), we discard $s$ and $e$ and start over.
\end{enumerate}

Since there are $2^{25 \times 25}$ different possible starting boards $s$, the data we can use is effectively infinite and is not a constraint on the quality of our model.

One convenient feature of this dataset is that each board is naturally a $25 \times 25$ matrix, which can be directly fed into a CNN as a single-channel image of width and height 25.

\section{Methods}
As mentioned in the Introduction, one of the inherent challenges in reversing the Game of Life is that this reversal is not one-to-one but one-to-many. For instance, take any board $s$ with a $3 \times 3$ region with all dead cells, you can replace the center cell in this region with a live cell, and get an $s'$ that evolves to the same $e$. The problem, in fact, is much more severe since typical evolved boards have mostly dead cells, and it's easy to see that there are many $3 \times 3$ boards whose cells all die after one evolution. This means that simply using the evolved boards $e$'s as the input and their corresponding starting boards $s$' as the output to train a supervised learning model will not work very well.

On the other hand, we note that the function $e$ to $s$ to $e$---reverse the evolution then evolve forward---is always one-to-one. This line of thought led to our first breakthrough, which we call the \emph{Reverse-Forward Network}.

\subsection{Reverse-Forward Network}
\label{revfwdnet}
The Reverse-Forward Network is a two-stage network consisting of a forward CNN $F$ that mimics the evolution in the Game of Life, and a reverse CNN $R$ that tries to reverse one evolution in the Game of Life. Now, given an evolved board $e$, we want $F(R(e))$ to equal to $e$---that is, first reversing $e$ to some $\hat s = R(e)$ then evolving $\hat s$ by $F$ should recover $e$. So the problem becomes training $F \circ R$ to predict $e$ from $e$. Now, in order to ensure $F \circ R$ does not reduce to the identity function, we fix the weights of $F$ during training of $R$, such that only the weights of latter are updated. This in theory should help $R$ to learn to reverse the Game of Life.

While we know of a three-layered CNN that perfectly replicates the evolution in Game of Life \cite{springer2020its}, using it in place of $F$ in the above algorithm yields poor results (as discussed in Section \ref{results}) since it assumes the inputs are binary, whereas the outputs of any $R$ are real values in $(0, 1)$ (and we cannot apply a step function to coerce the output of $R$ into binary values because the gradient would all go to 0 and not back-propagate through to the weights of $R$). In order to overcome this, we trained a \emph{Relaxed Forward Network} whose inputs are $25 \times 25$ matrices with floating point values between 0 and 1 that represent \emph{relaxed} or \emph{softened} versions of Game of Life boards, in the sense that cells with values in $[0, 0.5)$ are considered dead and cells with values in $[0.5, 1]$ are considered alive\footnote{The actual relaxation of the binary boards is done by replacing dead cells with values between $[0, 0.5)$ and live cells with values between $[0.5, 1]$, both draw uniformly randomly.}. We then train $F$ on these relaxed input boards to predict the evolved boards of their non-relaxed counterparts. Note this $F$ is taught to treat cells with value less than 0.5 as dead and cells with value greater or equal to 0.5 as alive, which is what we want.

To summarize, the Reverse-Forward Network is trained as follows:
\begin{enumerate}
    \item \textbf{Relaxed Forward Network}: We train $F$ to predict evolved boards on relaxed starting boards.
    \item \textbf{Reverse-Forward Network}: We fixed the weights of $F$ and train $F \circ R$ on non-relaxed boards $e$'s to recover $e$'s. (So we want $F \circ R$ to be the identity function.) The network $R$ can then learn to reverse the Game of Life.
\end{enumerate}

Only network $R$ is used during inference. In Figure \ref{fig:boards} we show an example of the performance of $R$ on an actual evolved board. 

\begin{figure}[!h]
\begin{center}
\minipage{0.15\textwidth}
  \includegraphics[width=\linewidth]{actual_end_board.png}
\endminipage\hfill
\minipage{0.15\textwidth}
  \includegraphics[width=\linewidth]{predicted_starting_board.png}
\endminipage\hfill
\minipage{0.15\textwidth}%
  \includegraphics[width=\linewidth]{predicted_stop_board.png}
\endminipage
\caption{The left board is the evolved board $e$ used as input to the Reverse-Forward Network; the board in the center is the prediction made by $R$ on $e$; the board on the right is the evolved board $\pi(R(e))$. The white cells are alive and the black cells are dead.}
\label{fig:boards}
\end{center}
\end{figure}


One deficiency of this approach is that the distribution of the live cells in the relaxed boards used to train $F$ is likely different from the output of $R$. This can be improved by retraining $F$ on this output of $R$. Improving on the Reverse-Forward Net led us to an approach inspired by Generative Adversarial Networks, which we refer to as \emph{Generative Collaborative Network}.

\subsection{Generative Collaborative Network}
\label{gcn}

\begin{figure}
    \centering
\includegraphics[width=0.5\textwidth]{GCN.png}
    \caption{Conceptual diagrams of our main learning approaches.}
    \label{fig:algo_concepts}
\end{figure}

Inspired by generative adversarial networks (GAN) \cite{goodfellow2014generative}, we propose an extension of the \emph{Reverse-Forward} approach, called \emph{Generative Collaborative Networks} (GCN).

In this method, training of both networks $F$ and $R$ (which are counterparts of the GAN discriminator and generator networks, $D$ and $G$ correspondingly) happen in a closed loop described in Algorithm \ref{gcn_alg}. In addition to evolved board $e$, $R$ also accepts the noise input $z$. Even though $z$ is not as critical in our problem as in, e.g., face image generation, the noise helps $R$ to generate a variety of starting boards $\hat s$ with higher confidence (more on the experimental results in Section \ref{results}). 

The key differences between GCN and GAN are:
\begin{itemize}
    \item The network $F$---contrary to GAN's discriminator---does not only output a single binary value predicting whether the input is real or fake, but the whole board $\hat e$.
    \item Calculating loss for network $F$ requires access to a simulator function $\pi \colon \{1,0\}^{n\times n} \rightarrow \{1,0\}^{n\times n}$---in our case, taking $s$ as input and returning ground truth $e$ as output. $\pi$ does not have to be differentiable (since it's only used to generate labels)---it can be even a hard-coded function implemented in any programming language.
    \item Networks $F$ and $R$, even though having different objectives, do not compete with each other. The equilibrium between networks' accuracy, as seeked in GANs, is not the goal here.
\end{itemize}

\begin{algorithm}
    \label{gcn_alg}
    \caption{Generative Collaborative Networks.}
    for number of training iterations do
    \begin{enumerate}
    \item Sample minibatch of $m$ noise samples $\{z^{(1)}
    , \ldots , z^{(m)}\}$ from noise prior $p(z)$, and $m$ examples $\{(s^{(1)}, e
    ^{(1)}), \ldots , (s^{(m)}, e^{(m)})\}$ from data source.
    \item For each example $i\in\{1,\ldots,m\}$:
    \begin{enumerate}
        \item Predict reverse $\hat s^{(i)} = R(e^{(i)}, z^{(i)})$
        \item Update parameters of $R$ by descending its gradient:
              $$\nabla_R \text{BCE}((F \circ R) (e^{(i)}, z^{(i)}), e^{(i)}) $$
              where $\text{BCE}$ stands for binary cross-entropy loss.
        \item Simulate step forward $\hat e_\pi^{(i)} = \pi(\hat s^{(i)})$
        \item Update parameters of $F$ by descending its gradient:
        $$\nabla_F \left(\text{BCE}(F(s^{(i)}), e^{(i)}) + \text{BCE}(F(\hat s^{(i)}), \hat e_{\pi}^{(i)})\right)$$
    \end{enumerate}
    \end{enumerate}
\end{algorithm} 

Note that if we run Algorithm \ref{gcn_alg} twice, with minor modifications, it reduces to the approach described in Section \ref{revfwdnet}:
\begin{itemize}
    \item In the first run, disable update 2.b) and change update 2.d) to just $\nabla_F \text{BCE}(F(s^{(i)}, e^{(i)}))$---effectively just training network $F$.
    \item In the second run, disable update 2.d) - effectively just training network $R$.
    \item Use empty noise $z$ or ignore it in $R$.
\end{itemize}

We embrace similarity between these algorithms and in practice, the best models we have obtained so far have used a mix of these approaches, as discussed further in Section \ref{results}.

\subsection{Model architectures}
\label{model_archs}
\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{RFN.png}
\caption{The CNN architectures used for the reverse network $R$ and the forward network $F$.}
\end{center}
\label{fig:RFNnetwork}
\end{figure}

In both learning approaches (see Sections \ref{revfwdnet} and \ref{gcn}) we have used similar network architectures based on convolutional neural networks. The best architectures, according to our experimental results, are presented in the diagram \ref{fig:RFNnetwork}.

To design the architecture for $F$, we started with the insights presented in \cite{springer2020its}. Note that $F$ does not require convolution filters larger than $3 \times 3$ and stride larger than $1$, since the forward rules of the Game of Life are based on just the 8 adjacent cells (see Section \ref{intro}). With the presented architecture, $F$ achieves nearly perfect accuracy on the forward prediction task, on the data generated according to the procedure described in Section \ref{dataset}.

The starting point which led us to the current architecture of $R$ was DCGAN \cite{radford2016unsupervised}, with more channels in the hidden layers, as well as an addition of batch normalization layers. The DCGAN's first layers of $D$ (which, as we reasoned, are responsible for ``understanding'' the input board $e$) and the last layers of $G$ (which are responsible for generating output $s$) are stacked together, forming $R$. Another major change while adapting that architecture to our problem was changing stride to $1$, which changed the DCGAN's funnel-shaped architecture into the current ``rectangular'' shape. 

The noise input $z \in \mathbb{R}^d$ is replicated $25 \times 25$ times (forming a tensor $\mathbb{R}^{d\times25\times25}$) and concatenated to the channels of one of the hidden layers of $R$.

As described in Section \ref{wraparoundboard}, the Game of Life's forward rules wrap around the edges of the board. Hence, in the convolution layers we use padding of type {\it circular}---offered out-of-box by PyTorch, but not present in many other deep learning frameworks (including TensorFlow), which impacted our decision to use PyTorch.


\section{Results and Discussion}
\label{results}
Table \ref{tab:eval_results} summarizes experimental results on a validation set containing 10K examples. The following sections will explain used metrics and how the baselines and the best models were produced.

\subsection{Evaluation}
\label{evaluation}
The evaluation criterion we use is the mean absolute error (MAE) when comparing the terminal state $\hat{e}$, reached from the model's prediction $\hat{s}$, to the ground-truth terminal state $e$. Mathematically, this is given by
$$\mathrm{MAE}_{\text{single}}(\hat e, e) = \frac{ \sum_{j=1}^{n}\sum_{k=1}^{n}|\hat e_{j,k} - e_{j,k}|}{n^2}$$

$$\mathrm{MAE}_{\text{dataset}} = \frac{\sum_{i=1}^{m} \mathrm{MAE}_{\text{single}}(\pi(\hat s^{(i)}), e^{(i)})}{m},$$

% old formula:
%$$\frac{\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=1}^{n}|x_{j,k}^{(i)} - \hat x_{j,k}^{(i)}|}{m \times n^2},$$

where $e^{(i)}$ is the $i$th ground truth terminal state and $\hat e^{(i)}$ is the terminal state evolved from the $i$th predicted starting board $\hat s^{(i)}$ using a simulator $\pi$. $m$ is number of examples in a data set and $n$ is the width/height of the board ($25$ by default).


\begin{table}
    \centering
    \caption{Evaluation results on a test set with 10k examples.}
    \label{tab:eval_results}

\begin{tabular}{rrr}
\hline
     Generator & MAE & error variance \\
\hline
 Constant zeros    & 14.72\%          & 0.79\% \\
 Likely starts  & 12.84\%          & 0.76\%     \\
 Our model $R_A$              & \textbf{4.23\%}           & 0.11\% \\
 %R\_multi        & 4.10\%           & 0.10\%          & 22.79\%            & 1.48\%            \\
 %R+zeros        & 4.23\%           & 0.11\%          & 12.54\%            & 0.80\%            \\
 %R+likely       & 4.23\%           & 0.11\%          & 12.35\%            & 0.83\%            \\
 \hline
 %Ensemble & \textbf{4.09\%}           & 0.10\%  \\
%\hline
\end{tabular}

\end{table}

\subsection{Trivial Baselines}
We established a couple of trivial baselines to get a sense for minimal metrics that more sophisticated approaches should beat:
\begin{itemize}
    \item {\it Constant zeros}: returning always a board of zeros.
    \item {\it Likely starts}: this simple heuristic tries forward simulation from 3 ``likely'' starting boards (constant zeros, a board equal to $e$, and a single step forward from $e$), then picks the one which achieves the highest score (per out evaluation metric) in the current example.
\end{itemize}

\subsection{Best $R$ Model}
Our best $R$ model, which we refer to as $R_A$ in Table \ref{tab:eval_results}, was obtained following the \emph{Reverse-Forward} strategy described in Section \ref{revfwdnet}, but including the noise input as described in Section \ref{gcn}. The model architecture was as described in Section \ref{model_archs}. We trained $R_A$ with the Adam optimizer \cite{kingma2017adam} on over 1.9M examples (generates in a streaming fashion as described in Section \ref{dataset}), with the optimization hyper-parameters following \cite{radford2016unsupervised} (see Figure \ref{fig:mae-loss}).

\begin{figure} %[!h]
    \centering
    \includegraphics[scale=0.8]{MAE-loss-hist-sm.png}
    \caption{Training history of model $R_A$. X-axis represents number of mini-batches (each of size 64) processed by the training loop. The upper chart shows MAE on a small validation set of 1024 examples (reported every 100 batches). The lower chart shows training loss of network $R$ (reported after every mini-batch). The vertical red line represents a point of the lowest MAE reported on the validation set, which led to our model selection for $R_A$.}
    \label{fig:mae-loss}
\end{figure}

\subsubsection{Choice of $F$} Figure \ref{fig:fwd_choices} summarizes the importance of pre-training a relaxed version of $F$. Despite its promise, updating $F$ continuously hasn't yielded expected gains yet.

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{MAE-different-Fs.png}
\caption{MAE on a validation set reported during training of $R$ with various choices of $F$, for the initial 80 epochs (6400 samples each). \emph{Exact F} is the network described in the appendix of \cite{springer2020its}.}
        \label{fig:fwd_choices}
\end{figure}

\subsubsection{Noise input}
Based on our experiments, the noise input $z$ slightly improves performance of the $R$ network (see Figure \ref{fig:noise}). This is likely due to the fact every evolved board $e$ may have many starting boards $s$, and an additional noise input allows $R$ to avoid averaging between multiple possible predictions.

We experimented with noise distribution priors $p(z) = \mathcal{N}(0,1)$ and $p(z) = \mathcal{U}(0,1)$, as well as different sizes of sampled vector $z$. We obtained the best results with $z \in \mathbb{R}^8 \sim \mathcal{U}(0,1)$. We hypothesize that the uniform distribution works better because the data is also generated using uniform random number generator (see Section \ref{dataset}).

The added benefit of having input $z$ is that it allows to generate many different boards $s$ given the same input $e$, which can be helpful in creating an ensemble model.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.8]{MAE-uniform-noise.png}
    \caption{MAE on a validation set reported during training of $R$ with and without uniform noise input $z$ (otherwise the architecture and hyper-parameters were the same in both cases) for the initial 100 epochs (where 1 epoch = 6400 examples).}
    \label{fig:noise}
\end{figure}


\subsubsection{Promise of GCN}
\label{promise}
As we mentioned before, to train the winning model $R_A$, we first trained $F$ until convergence, then $R$ until convergence. Updating $F$ in a loop as outlined in algorithm \ref{gcn_alg} hasn't yielded improved outcomes in our experiments so far. However, we speculate that GCN may still provide value after careful hyper-parameter tuning. The reason for this suspicion is based on analysis of Figure \ref{fig:mae-loss}, representing training history of $R_A$. We observe smooth and stable improvement in both MAE and loss in the initial phase of training (left side of the vertical red line), which suddenly changes to a significant improvement in loss and degradation in MAE at the same time (right side of the vertical red line). This suggests that $R$ started overfitting to $F$. Updating $F$ continuously to better handle $R$'s output distribution may prevent this problem.


\subsubsection{Impact of weight initialization}
We observed that running an experiment with the same hyper-parameters multiple times, each time with a different random seed, often leads to significantly different final metrics. This may suggest that the model architectures we have experimented with so far are not large enough.\cite{frankle2019lottery}

\subsection{Other Experiments}

\subsubsection{GAN}
We have also tried using GANs directly, i.e., a training loop with a discriminator network $D$ (instead of $F$) learning to predict whether board $s$ comes from ground-truth or is generated by $R$ ($G$ in GANs terminology). While the GAN training was making progress, and the generated boards looked more and more similar to the real Game of Life boards, it didn't translate into competitive MAE metric. We hypothesized that back-propagation from a single binary cross-entropy loss per example doesn't provide specific enough information to $R$ about which parts of the board were wrong. Replacing $D$ with the forward network $F$ (hence producing GCN), allows the back-propagation to provide more specific signal to $R$ about the parts of the prediction $\hat s$ that are most likely wrong.

On the other hand, making sure that $R$ generates boards $\hat s$ from a similar distribution as ground truth boards $s$ might help avoid problems with overfitting to $F$ (see Section \ref{promise}). We intend to experiment with approaches involving both $D$ and $F$ in the same training loop.

\subsubsection{Probabilistic model}
Another direction we explored is to model the distribution $\mathbf P(s | e)$ for boards of various sizes. For this, we learned empirical probabilities of $\mathbf P(s_{jk} | E_{jk})$ where $s_{jk}$ is the $(j, k)$th cell in the starting board and $E_{jk}$ is the $3 \times 3$ tile centered at ($j, k$) on the evolved board. For this, we generated 200,000 parent board--evolved board pairs using our data generator and used them as the inputs to generate these empirical probabilities. 

Once we have the empirical probabilities $p = \mathbf P(s_{jk} | E_{jk})$, we can make predictions (or more precisely, educated guesses) of the parent board by looping through every $3 \times 3$ tile $E_{jk}$ in the evolved board and sampling the corresponding center cell $s_{ij}$ in the parent board using a Bernoulli distribution with parameter $p$. But in the end the CNN-based deep learning approaches proved to be superior.

\section{Conclusion and Future Work}

Reversing the Game of Life is a challenging and very interesting problem which can serve as a benchmark for techniques recovering an inverse of a known, but complex and non-linear function.

We believe approaches developed on such benchmark could be applicable to some real-world applications such as
\begin{itemize}
    \item \textbf{Signal processing}: Learning a function to revert an arbitrary operation on the signal. E.g. in case of audio processing, one could imagine a task of removing reverb from an already-processed audio file. 
    \item \textbf{Recovering information}: Learning a function to reverse lossy compression on some class of documents.
\end{itemize}

The future work includes:
\begin{itemize}
    \item Combating the problem of $R$ overfitting to $F$ (see Section \ref{promise}) by fine-tuning the GCN approach.
    \item Experimenting with different model architectures, including larger models.
    \item Generalizing the solution to reversing the Game of Life by multiple steps $\delta$ (as discussed in \ref{intro}) with low MAE.
\end{itemize}

\section{Contributions}
Besides all authors contributing to writing the report, their individual contributions included:

\begin{itemize}
    \item \textbf{Johnson Jia} worked on the Reverse-Forward Network, helped with setting up the Tensorboard and came up with the idea of relaxing the input to the forward network and fixing the weights when training both reverse and forward networks together. He also implemented the simple probabilistic model.

    \item \textbf{Sebastian Kochman} developed and conducted experiments with the GCN approach. He implemented streaming data generation and evaluation. He also worked on the algorithmic approaches which were discussed in the milestone paper.

    \item \textbf{Jianyu Lu} worked on the Kaggle competition besides editing the report. He helped with implementing the reverse convolutional neural network and migrated other models to the Kaggle notebooks to handle multi-steps evolution of cells.

\end{itemize}


%\bibliographystyle{plain}
%\bibliographystyle{acl_natbib}
\bibliographystyle{IEEEtran}
\bibliography{final_report}

\end{document}
