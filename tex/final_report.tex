\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[ruled]{algorithm2e}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\newcommand\todo[1]{\textcolor{red}{#1}}

\renewcommand{\thesection}{\arabic{section}}
\def\thesectiondis{\thesection.} \def\thesubsectiondis{\thesectiondis\arabic{subsection}.} \def\thesubsubsectiondis{\thesubsectiondis\arabic{subsubsection}.} \def\theparagraphdis{\thesubsubsectiondis\arabic{paragraph}.}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\begin{document}

\title{Reversing Conway's Game of Life}

%\icmlcorrespondingauthor{}{johnjia@stanford.edu}
%\icmlcorrespondingauthor{Sebastian Kochman}{sebastko@stanford.edu}
%\icmlcorrespondingauthor{Jianyu Lu}{jylux@stanford.edu}
\author{\IEEEauthorblockN{Johnson Jia}
\IEEEauthorblockA{johnjia@stanford.edu}
\and
\IEEEauthorblockN{Sebastian Kochman}
\IEEEauthorblockA{sebastko@stanford.edu}
\and
\IEEEauthorblockN{Jianyu Lu}
\IEEEauthorblockA{jylux@stanford.edu}
}

\maketitle

\begin{abstract}
The Game of Life \cite{b2} is a cellular automaton created by the mathematician John Conway. It consists of a board of cells that are either live or dead, such that each cell evolves according to a set of deterministic rules  based on the state of the its neighboring cells. We apply machine learning to reverse the Game of Life, that is, to generate one of the many preceding states that evolve to a given state.
\end{abstract}

\section{Introduction}
The Game of Life is a \emph{life-like} cellular automaton \cite{lifelike} where a board of live or dead cells evolve according to the following two simple rules:
\begin{enumerate}
    \item A live cell that is surrounded by two or three lives cells survives in the next round, otherwise it dies.
    \item A dead cell that is surrounded by three living cells comes alive the next round, otherwise it stays dead.
\end{enumerate}
Whereas this forward evolution in the Game of Life can be modeled perfectly using a convolutional neural network---in fact, one such CNN is prescribed in the appendix of  \cite{b5}---learning to reverse the Game of Life is much more challenging. For one, many different boards can evolve to the same board, so a supervised learning approach with the evolved boards as the input and their predecessors---that is, the boards from which they evolved---as the output is unlikely to work. In this paper, we describe an approach which partially overcomes this one-to-many problem when reversing the Game of Life.

As the idea for this project came from a Kaggle competition \cite{b1}, we adopt many of the problem settings outlined in the competition (with some minor differences)\footnote{In particular, the competition requires reversing multiple steps whereas we solely focus on reversing just one step in this paper.}. \label{wraparoundboard} We will focus on boards of size $25 \times 25$, with wrap-around in the sense that cells on the left-edge (top-edge) are considered adjacent to the cells on the right-edge (bottom-edge) in the same row (column). We also focus on reversing just one step of the Game of Life, as its difficulty is representative of the general problem of reversing $\delta$ steps, and a model that does well in this special case should in theory be easy to generalize to the general case\footnote{Although in actuality we noticed that minor errors in the one-step reversal get amplified significantly during repeated application of the model to reverse multiple steps. We leave this generalization work for the future.}.

\subsection{Notation}
In order to aid the discussion, we adopt the following notation:

\begin{itemize}
    \item $S, E \in \{0, 1\}^{25\times 25}$: matrices with entries in $0$ (dead or off) or $1$ (live or on) representing states (or ``boards'') in the Game of Life: $S$ denotes the starting board and $E$ denotes a corresponding one-step evolution, which we refer to as the \emph{evolved} or stop board.
    \item $S^{(i)}, E^{(i)}$ - superscript $(i)$ denotes an $i$th example of evolution in some data set.
    \item $s_{j,k}$ or $e_{j,k}$: the value of the cell in the $j$th row and $k$th column of $S$ or $E$ correspondingly. We consider the boards to be wrapped around the edges, so each cell has exactly 8 neighbors (or adjacent cells).
    \item When applicable we'll use hat (e.g. $\hat S$) to denote predicted outputs.
\end{itemize}

Using this notation, our goal is to train a model that, given an evolved board $E^{(i)}$, predicts one of the (potentially) many $S^{(i)}$'s that evolve to a given $E^{(i)}$.

\section{Related Work}
In \cite{b5}, the authors worked on training a neural network to predict the forward evolution. This is a much simpler solution with a known solution---a three-layered CNN---alluded to earlier. The authors however showed that training such a three-layered CNN from scratch fails to learn the forward evolution, but instead one has to increase the size of the model fairly substantially in order to learn it with perfect accuracy.

Game of Life's forward function has been studied in the context of teaching convolution neural networks (\todo{cite}).

In principle, the problem we study is finding an inverse of a complex non-linear function.  Independent component analysis (ICA)  is an unsupervised method of finding a reverse of linear transformation that produced observed data. There has been recently a lot of progress in studying non-linear ICA (cite https://arxiv.org/abs/1805.08651 , https://arxiv.org/abs/1710.05050 ), which often involve generative adversarial nets (GANs cite https://arxiv.org/abs/1406.2661). In our work, we study applicability of GANs to reversing Game of Life.

\section{Dataset}

We created a data generator (following the procedure prescribed by the aforementioned Kaggle competition) to generate the datasets used for training and evaluation.

\begin{enumerate}
\item An initial board is chosen by filling the board with a random density of live cells between 1\% (mostly 0's) and 99\% (mostly 1's).
\item This initial board is evolved 5 steps to ``warm up'' the board. This is partly to evolve the board into a state that's more ``life-like''. This post-warm-up board will be a starting board $S$.
\item We then evolve $S$ one step to get $E$. If the stopping board turns out to be empty, we discard $S$ and $E$ and start over.
\end{enumerate}

Since there are $2^{25 \times 25}$ different possible $B$'s, the data we can use is effectively infinite and is not a constraint on the quality of our model\footnote{With that said, our compute and memory resources are limited so we restricted our training dataset to 128000 start-stop board pairs}. 

One convenient feature of this dataset is that each board is naturally a $25 \times 25$ matrix, which can be directly fed into a CNN as a single-channel image of width and height 25.

\section{Methods}
As mentioned in the introduction, one of the inherent challenges in reversing the Game of Life is that this reversal is not one-to-one but one-to-many. For instance, take any board $S$ with a $3 \times 3$ region with all dead cells, you can replace the center cell in this region with a live cell, and get an $S'$ that evolves to the same $E$. The problem, in fact, is much more severe since typical evolved boards have mostly dead cells, and it's easy to see that there are many $3 \times 3$ boards whose cells all die after one evolution. This means that simply using the evolved boards $E$'s as the input and their corresponding starting boards $S$' as the output to train a supervised learning model will not work very well.

On the other hand, we note that the function $E$ to $S$ to $E$---reverse the evolution then evolve forward---is always one-to-one. This line of thought led to our first breakthrough, which we call the \emph{Reverse-Forward Network}.

\subsection{Reverse-Forward Network}
\label{revfwdnet}
The Reverse-Forward Network is a two-stage network consisting of a forward CNN $F$ that mimics the evolution in the Game of Life, and a reverse CNN $R$ that tries to reverse one evolution in the Game of Life. Now, given an evolved board $E$, we want $F(R(E))$ to equal to $E$---that is, first reversing $E$ to some $\hat B = R(E)$ then evolving $\hat B$ by $F$ should recover $E$. So the problem becomes training $F \circ R$ to predict $E$ from $E$. Now, in order to ensure $F \circ R$ does not reduce to the identity function, we fix the weights of $F$ during training, such that only the weights of $R$ are updated. This in theory should help $R$ to learn to reverse the Game of Life.

While we know of a three-layered CNN that perfectly replicates the evolution in Game of Life \cite{b5}, it cannot be used as the $F$ in the above algorithm since it assumes the inputs are binary\footnote{We did try using this CNN as the F and the results were poor. Also we noted that its behavior on non-binary inputs is unpredictable}, whereas the output of any $R$ is necessarily non-binary (and we cannot apply a step function to coerce the output of R into binary values because the gradient will all go to 0 and not back-propagate through to the weights of $R$). In order to overcome this, we trained a \emph{Relaxed Forward Network} whose inputs are $25 \times 25$ matrices with floating point values between 0 and 1 that represent \emph{relaxed} or \emph{softened} versions of Game of Life boards, in the sense that cells with values in $[0, 0.5)$ are considered dead and cells with values in $[0.5, 1]$ are considered alive. We then train $F$ on these relaxed input boards to predict the evolved boards of their non-relaxed counterparts. Note this $F$ is taught to treat cells with value less than 0.5 as dead and cells with value greater or equal to 0.5 as alive, which is what we want.

To summarize, the Reverse-Forward Network is trained as follows:
\begin{enumerate}
    \item \textbf{Relaxed Forward Network}: We train $F$ to predict evolved boards on relaxed starting boards.
    \item \textbf{Reverse Forward Network}: We fixed the weights of $F$ and train $F \circ R$ on non-relaxed boards $E$'s to recover $E$'s. (So we want $F \circ R$ to be the identity function.) The network $R$ can then learn to reverse the Game of Life.
\end{enumerate}

Only network $R$ is used during inference.

The architecture for the Reverse-Forward Network is shown in \todo{\ref{Reverse-Forward architecture}}.

One deficiency of this approach is that the distribution\footnote{We used a uniform distribution on $[0, 1]$ to generated the relaxed boards.} of the live cells in the relaxed boards used to train $F$ is likely different from the output of $R$. This can be improved by retraining $F$ on this output of $R$. Improving on the Reverse-Forward Net led us to an approach inspired by Generative Adversarial Networks, which we refer to as \emph{Generative Collaborative Network}.

\subsection{Generative Collaborative Network}
\label{gcn}
Inspired by generative adversarial networks (GAN, \todo{(cite)}), we propose an extension of the \emph{Reverse-Forward} approach, called \emph{Generative Collaborative Networks} (GCN).

% I don't know how to use bibtex:
%~\cite{goodfellow2014generative}

In this method, training of both networks $F$ and $R$ (which are counterparts of the GAN discriminator and generator networks - $D$ and $G$ - correspondingly) happen in a closed loop described in algorithm \ref{gcn_alg}. In addition to evolved board $e$, $R$ also accepts the noise input $z$. Even though $z$ is not as critical in our problem as in, e.g., face image generation, the noise helps $R$ to generate a variety of starting boards $\hat s$ with higher confidence (more on the experimental results in section \ref{results}). 

The key differences between GCN and GAN are:
\begin{itemize}
    \item The network $F$ - contrary to GAN's discriminator - does not only output a single binary value predicting whether the input is real or fake, but the whole board $E$.
    \item Calculating loss for network $F$ requires access to a simulator function $\pi : \{1,0\}^{n\times n} \rightarrow \{1,0\}^{n\times n}$ - in our case, taking $S$ as input and returning ground truth $E$ as output. $\pi$ does not have to be differentiable - it can be even a hard-coded function implemented in any programming language.
    \item Networks $F$ and $R$, even though having different objectives, do not compete with each other. The equilibrium between networks' accuracy, as seeked in GANs, is not the goal here.
\end{itemize}

\begin{algorithm}
    \label{gcn_alg}
    \caption{Generative Collaborative Networks.}
    for number of training iterations do
    \begin{enumerate}
    \item Sample minibatch of $m$ noise samples $\{z^{(1)}
    , . . . , z^{(m)}\}$ from noise prior $p(z)$, and $m$ examples $\{(S^{(1)}, E
    ^{(1)}), . . . , (S^{(m)}, E^{(m)})\}$ from data source.
    \item For each example $i\in\{1,...,m\}$:
    \begin{enumerate}
        \item Predict reverse $\hat S^{(i)} = R(E^{(i)}, z^{(i)})$
        \item Simulate step forward $\hat E_\pi^{(i)} = \pi(\hat S^{(i)})$
        \item Update parameters of $R$ by descending its gradient:
        
        $$\nabla_R \text{BCE}((F \circ R) (E^{(i)}, z^{(i)}), E^{(i)}) $$
        
        where $\text{BCE}$ stands for binary cross-entropy loss.
        
        \item Update parameters of $F$ by descending its gradient:
        
        $$\nabla_F \left(\text{BCE}(F(S^{(i)}), E^{(i)}) + \text{BCE}(F(\hat S^{(i)}), \hat E_{\pi}^{(i)})\right)$$
    \end{enumerate}
    \end{enumerate}
\end{algorithm} 

Note that if we run algorithm \ref{gcn_alg} twice, with minor modifications, it reduces to the approach described in section \ref{revfwdnet}:
\begin{itemize}
    \item In the first run, disable update 2.c) and change update 2.d) to just $\nabla_F \text{BCE}(F(S^{(i)}, E^{(i)}))$ - effectively just training network $F$.
    \item In the second run, disable update 2.d) - effectively just training network $R$.
    \item Use empty noise $z$ or ignore it in $R$.
\end{itemize}

We embrace similarity between these algorithms and in practice, the best models we have obtained so far have used a mix of these approaches, as discussed further in section \ref{results}.

\subsection{Model architectures}
In both learning approaches (see \ref{revfwdnet}, \ref{gcn}) we have used similar network architectures based on convolutional neural networks.

\todo{Jianyu's diagrams/explanation of the architecture}

For the models $R$ accepting additional noise input $z \in \mathbb{R}^d$, the $z$ vector is replicated $25 \times 25$ times (forming a tensor $\mathbb{R}^{d\times25\times25}$) and concatenated to the channels of one of the hidden layers of $R$.

As described in section \ref{wraparoundboard}, the GoL's forward rules wrap around the edges of the board. Hence, in the convolution layers we use padding of type {\it circular} - offered out-of-box by PyTorch, but not present in many other deep learning frameworks (including Tensorflow), which impacted our decision to use PyTorch.


\section{Results and Discussion}
\label{results}
Table \ref{results_table} summarizes experimental results on a validation set containing 10K examples. The following sections will explain used metrics and how the baselines and the best models were produced.

\subsection{Evaluation}
\label{evaluation}
The evaluation criterion we use is the mean absolute error (MAE) when comparing the terminal state $\hat{E}$, reached from the model's prediction $\hat{S}$, to the ground-truth terminal state $E$. Mathematically, this is given by
$$\mathrm{MAE}_{\text{single}}(\hat E, E) = \frac{ \sum_{j=1}^{n}\sum_{k=1}^{n}|\hat E_{j,k} - E_{j,k}|}{n^2}$$

$$\mathrm{MAE}_{\text{dataset}} = \frac{\sum_{i=1}^{m} \mathrm{MAE}_{\text{single}}(\pi(\hat S^{(i)}), E^{(i)})}{m},$$

% old formula:
%$$\frac{\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=1}^{n}|x_{j,k}^{(i)} - \hat x_{j,k}^{(i)}|}{m \times n^2},$$

where $E^{(i)}$ is the $i$th ground truth terminal state and $\hat E^{(i)}$ is the terminal state evolved from the $i$th predicted starting board $\hat S^{(i)}$ using a simulator $\pi$. $m$ is number of examples in a data set and $n$ is the width/height of the board ($25$ by default).


\begin{table*}[t]
    \label{results_table}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & \multicolumn{2}{|c|}{Multi-step error ($\delta \in \{1,\dots,5\}$)} & \multicolumn{2}{|c|}{Single-step error ($\delta = 1$)} \\
    \hline
    Model & mean & variance & mean & variance \\
    \hline
    Constant zeros & 14.72\% & 0.79\% & 14.72\% & 0.79\% \\
    Likely starts & \textbf{14.15\%} & 0.83\% & 12.84\% & 0.76\% \\
    Reverse v1 & 14.92\% & 1.19\% & \textbf{6.35\%} & 0.23\% \\
    \hline
    Ensemble A & 14.17\% & 1.08\% & 6.16\% & 0.22\% \\
    Ensemble B & \textbf{12.35\%} & 0.78\% & \textbf{6.14\%} & 0.22\% \\
    \hline
    \end{tabular}
    \caption{Evaluation results on a validation set with 10k examples.}
    \label{tab:eval_results}
\end{table*}

\subsection{Trivial Baselines}
We established a couple of trivial baselines to get a sense for minimal metrics that more sophisticated approaches should beat:
\begin{itemize}
    \item {\it Constant zeros} - returning always a board of zeros.
    \item {\it Likely starts} - this simple heuristic tries forward simulation from 3 ``likely'' starting boards (constant zeros, a board equal to $e$, and a single step forward from $e$), then picks the one which achieves the highest score (per out evaluation metric) in the current example.
\end{itemize}


\section{Conclusion and Future Work}


\section{Contributions}
The authors contributed equally to the project: %We outline their individual contributions below.
\begin{itemize}
    \item \textbf{Johnson Jia} implemented the simple probabilistic model and contributed to writing and editing the project proposal as well as this milestone report.

    \item \textbf{Sebastian Kochman} implemented skeleton code for data generation and evaluation, evaluated baseline approaches, tile graph-based heuristics and the CNN V1 model, as well as contributed to writing the milestone report.

    \item \textbf{Jianyu Lu} implemented a simple CNN which achieves similar results to CNN V1, completed and submitted the team's first Kaggle notebook.

\end{itemize}

\begin{thebibliography}{00}         
\bibitem{b1} ``Conway's Reverse Game of Life 2020,'' \url{https://www.kaggle.com/c/conways-reverse-game-of-life-2020/overview}
\bibitem{b2} ``Conway's Game of Life,'' Wikipedia, \url{https://en.wikipedia.org/wiki/Conway\%27s_Game_of_Life}
\bibitem{lifelike} ``Life-like Cellular Automaton,'' Wikipedia, \url{https://en.wikipedia.org/wiki/Life-like_cellular_automaton}
\bibitem{b3} ``Going Deeper with Convolutions'' \url{https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf}
\bibitem{b4} ``Deep Residual Learning for Image Recognition''
\url{https://arxiv.org/abs/1512.03385}
\bibitem{b5} ``It’s Hard For Neural Networks to Learn the Game of
Life''
\url{https://arxiv.org/pdf/2009.01398.pdf}

\bibitem{gan} ``Advances in Neural Information Processing Systems''
\url{https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf}
\author[]{Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua}

% bibtex:
\iffalse
@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
\fi

\end{thebibliography}

\end{document}
