{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'..')\n",
    "import bitmap\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or load training data\n",
    "N = 51200\n",
    "\n",
    "if REGENERATE_DATA:\n",
    "    train_data_gen = bitmap.generate_train_set(N, 41, min_delta=1, max_delta=1)\n",
    "    deltas, start_boards, stop_boards = map(np.array, zip(*list(train_data_gen)))\n",
    "    # Save training data\n",
    "    np.save('../../data/training_start_boards', start_boards)\n",
    "    np.save('../../data/training_stop_boards', stop_boards)\n",
    "else:\n",
    "    start_boards = np.load('../../data/training_start_boards.npy')\n",
    "    stop_boards = np.load('../../data/training_stop_boards.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or load validation data\n",
    "N_valid = 12800\n",
    "\n",
    "if REGENERATE_DATA:\n",
    "    valid_data_gen = bitmap.generate_train_set(N_valid, 1024, min_delta=1, max_delta=1)\n",
    "    deltas, valid_start_boards, valid_stop_boards = map(np.array, zip(*list(valid_data_gen)))\n",
    "    # Save validation data\n",
    "    np.save('../../data/valid_start_boards', valid_start_boards)\n",
    "    np.save('../../data/valid_stop_boards', valid_stop_boards)\n",
    "else:\n",
    "    valid_start_boards = np.load('../../data/valid_start_boards.npy')\n",
    "    valid_stop_boards = np.load('../../data/valid_stop_boards.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = Variable(torch.tensor(valid_start_boards).view(N_valid, 1, 25, 25).float())\n",
    "y_valid = Variable(torch.tensor(valid_stop_boards).view(N_valid, 1, 25, 25).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or load test data\n",
    "N_test = 25600\n",
    "\n",
    "if REGENERATE_DATA:\n",
    "    test_data_gen = bitmap.generate_train_set(N_test, 42, min_delta=1, max_delta=1)\n",
    "    deltas, test_start_boards, test_stop_boards = map(np.array, zip(*list(test_data_gen)))\n",
    "    # Save test data\n",
    "    np.save('../../data/test_start_boards', test_start_boards)\n",
    "    np.save('../../data/test_stop_boards', test_stop_boards)\n",
    "else:\n",
    "    test_start_boards = np.load('../../data/test_start_boards.npy')\n",
    "    test_stop_boards = np.load('../../data/test_stop_boards.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward evolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_forward(x, use_cuda=False): \n",
    "    # Weights for layer 1\n",
    "    weight1 = torch.tensor([[[1, 1, 1], [1, 0.1, 1], [1, 1, 1]],\n",
    "                            [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]).view(2, 1, 3, 3).float()\n",
    "    b1 = torch.tensor([-3, -2]).float()\n",
    "    # Weights for layer 2\n",
    "    weight2 = torch.tensor([-10, 1]).view(1, 2, 1, 1).float()\n",
    "    # Weights for layer 3\n",
    "    s = 20\n",
    "    weight3 = torch.tensor([2*s]).view(1, 1, 1, 1).float()\n",
    "    b3 = torch.tensor([-s]).float()\n",
    "    \n",
    "    if use_cuda:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        weight1 = weight1.cuda()\n",
    "        b1 = b1.cuda()\n",
    "        weight2 = weight2.cuda()\n",
    "        weight3 = weight3.cuda()\n",
    "        b3 = b3.cuda()\n",
    "\n",
    "    x = F.pad(x.float(), (1, 1, 1, 1), mode='circular')\n",
    "    x = F.relu(F.conv2d(x, weight1, b1))\n",
    "    x = F.relu(F.conv2d(x, weight2))\n",
    "    x = torch.sigmoid(F.conv2d(x, weight3, b3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, X_valid, y_valid, \n",
    "          optim, criterion, output_path, num_epochs=50, batch_size=128):\n",
    "    # Release CUDA memory\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    # Set optimizer\n",
    "    optimizer = optim(model.parameters())\n",
    "    \n",
    "    # Setup Tensorboard (https://pytorch.org/docs/stable/tensorboard.html)\n",
    "    writer = SummaryWriter()\n",
    "    # writer.add_graph(model.cpu(), X)\n",
    "    model.cuda()\n",
    "\n",
    "    # Best validation MAE\n",
    "    best_valid_mae = 1\n",
    "    \n",
    "    # Train\n",
    "    n_iter = 0\n",
    "    for epoch in range(num_epochs): \n",
    "        permutation = torch.randperm(X.size()[0])\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(range(0, X.size()[0], batch_size))\n",
    "        for i in pbar:\n",
    "            n_iter += 1\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch = X[indices].cuda()\n",
    "            target = y[indices].cuda()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate MAE\n",
    "            if hasattr(model, \"reverse_net\"):\n",
    "                pred_start_boards = model.reverse_net(batch)\n",
    "                outputs = actual_forward((pred_start_boards > 0.5).int(), True)\n",
    "            output_boards = (outputs > 0.5).int()\n",
    "            mae = torch.sum(output_boards != target).float() / (batch_size * 25 * 25)\n",
    "            \n",
    "            # Write data to Tensorboard\n",
    "            writer.add_scalar('Loss/train', loss.item(), n_iter)\n",
    "            writer.add_scalar('MAE/train', mae.item(), n_iter)\n",
    "            \n",
    "            pbar.set_description(\"[{:d}, {:5d}] loss: {:.6f} | train MAE {:.6f} | best MAE: {:.6f}\".format(epoch + 1, i + 1, loss.item(), mae, best_valid_mae))\n",
    "            \n",
    "            # Write boards and validation results to Tensorboard every 50 batches\n",
    "            if n_iter % 50 == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    valid_loss = 0\n",
    "                    valid_mae = 0\n",
    "                    m = 0\n",
    "                    for j in range(0, X_valid.size()[0], batch_size):\n",
    "                        m += 1\n",
    "                        valid_batch = X_valid[j:j+batch_size].cuda()\n",
    "                        valid_target = y_valid[j:j+batch_size].cuda()\n",
    "                        valid_outputs = model(valid_batch)\n",
    "                        valid_loss += criterion(valid_outputs, valid_target)\n",
    "                        if hasattr(model, \"reverse_net\"):\n",
    "                            valid_start_boards = model.reverse_net(valid_batch)\n",
    "                            valid_outputs = actual_forward((valid_start_boards > 0.5).int(), True)\n",
    "                        valid_boards = (valid_outputs > 0.5).int()\n",
    "                        valid_mae += torch.sum(valid_boards != valid_target).float()\n",
    "                    valid_loss /= m\n",
    "                    valid_mae /= (X_valid.size()[0] * 25 * 25)\n",
    "                    writer.add_image('predicted stop board', valid_boards[-1], n_iter)\n",
    "                    writer.add_image('actual stop board', y_valid[-1], n_iter)\n",
    "                    if hasattr(model, \"reverse_net\"):\n",
    "                        pred_start_board = (model.reverse_net(X_valid[-1].view(1, 1, 25, 25).cuda()) > 0.5).int()\n",
    "                        writer.add_image('predicted start board', pred_start_board[-1], n_iter)\n",
    "                    writer.add_scalar('Loss/valid', valid_loss.item(), n_iter)\n",
    "                    writer.add_scalar('MAE/valid', valid_mae.item(), n_iter)\n",
    "                    \n",
    "                if valid_mae < best_valid_mae:\n",
    "                    best_valid_mae = valid_mae\n",
    "                    # Save model if we have the lastest best MAE\n",
    "                    torch.save(model.state_dict(), output_path)\n",
    "    writer.close()\n",
    "    print(\"The best validation MAE: {}\".format(best_valid_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forward_mae(model, weight_path, input_boards, output_boards, n):\n",
    "    # Release CUDA memory\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load model\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    model.cuda()\n",
    "    # Convert boards to tensor\n",
    "    input_boards_tensor = torch.tensor(input_boards[:n]).view(n, 1, 25, 25).float().cuda()\n",
    "    output_boards_tensor = torch.tensor(output_boards[:n]).view(n, 1, 25, 25)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Make prediction\n",
    "        if hasattr(model, \"reverse_net\"):\n",
    "            predicted_start_board = model.reverse_net(input_boards_tensor)\n",
    "            predicted_output_board = (actual_forward.forward((predicted_start_board > 0.5).int()) > 0.5).int()\n",
    "        else:\n",
    "            predicted_output_board = (model(input_boards_tensor) > 0.5).int()\n",
    "        error = torch.sum(predicted_output_board.cpu() != output_boards_tensor)\n",
    "        # print(predicted_stop_board)\n",
    "        # print(stop_boards_tensor)\n",
    "        return error / (n * 25 * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relax starting boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify starting boards \n",
    "def relax_boards(boards):\n",
    "    np.random.seed(41)\n",
    "    return np.abs(np.random.rand(*boards.shape) / 2 - boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "relaxed_start_boards = relax_boards(start_boards)\n",
    "relaxed_valid_start_boards = relax_boards(valid_start_boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelaxedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelaxedForwardNet, self).__init__()\n",
    "        # in channels, out channels, kernel size\n",
    "        self.conv0 = nn.Conv2d(1, 8, (1, 1))\n",
    "        self.activ0 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(8, 16, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ1 = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 8, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ2 = nn.PReLU()\n",
    "        self.conv3 = nn.Conv2d(8, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ3 = nn.PReLU()\n",
    "        self.conv4 = nn.Conv2d(4, 1, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activ0(self.conv0(x))\n",
    "        x = self.activ1(self.conv1(x))\n",
    "        x = self.activ2(self.conv2(x))\n",
    "        x = self.activ3(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "relaxed_forward_net = RelaxedForwardNet()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "relaxed_forward_model_path = \"../models/johnson/relaxed_forward.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse model version A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseNetA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReverseNetA, self).__init__()\n",
    "        # in channels, out channels, kernel size\n",
    "        self.conv0 = nn.Conv2d(1, 4, (1, 1))\n",
    "        self.activ0 = nn.ReLU()\n",
    "        self.conv1_7 = nn.Conv2d(4, 4, (7, 7), padding=(3, 3), padding_mode='circular')\n",
    "        self.conv1_5 = nn.Conv2d(4, 4, (5, 5), padding=(2, 2), padding_mode='circular')\n",
    "        self.conv1_3 = nn.Conv2d(4, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.conv1_1 = nn.Conv2d(4, 4, (1, 1))\n",
    "        self.activ1 = nn.PReLU()\n",
    "        self.conv2_5 = nn.Conv2d(16, 4, (5, 5), padding=(2, 2), padding_mode='circular')\n",
    "        self.conv2_3 = nn.Conv2d(16, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ2 = nn.PReLU()\n",
    "        self.conv3 = nn.Conv2d(8, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ3 = nn.PReLU()\n",
    "        self.conv4 = nn.Conv2d(4, 1, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activ0(self.conv0(x))\n",
    "        x = self.activ1(torch.cat((self.conv1_1(x), self.conv1_3(x), \n",
    "                                   self.conv1_5(x), self.conv1_7(x)), 1))\n",
    "        x = self.activ2(torch.cat((self.conv2_3(x), self.conv2_5(x)), 1))\n",
    "        x = self.activ3(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse model version B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseNetB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReverseNetB, self).__init__()\n",
    "        # in channels, out channels, kernel size\n",
    "        self.conv0 = nn.Conv2d(1, 4, (1, 1))\n",
    "        self.activ0 = nn.ReLU()\n",
    "        self.conv1_5 = nn.Conv2d(4, 8, (5, 5), padding=(2, 2), padding_mode='circular')\n",
    "        self.conv1_3 = nn.Conv2d(4, 8, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ1 = nn.PReLU()\n",
    "        self.conv2_5 = nn.Conv2d(16, 4, (5, 5), padding=(2, 2), padding_mode='circular')\n",
    "        self.conv2_3 = nn.Conv2d(16, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ2 = nn.PReLU()\n",
    "        self.conv3 = nn.Conv2d(8, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ3 = nn.PReLU()\n",
    "        self.conv4 = nn.Conv2d(4, 1, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activ0(self.conv0(x))\n",
    "        x = self.activ1(torch.cat((self.conv1_3(x), self.conv1_5(x)), 1))\n",
    "        x = self.activ2(torch.cat((self.conv2_3(x), self.conv2_5(x)), 1))\n",
    "        x = self.activ3(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse model version C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseNetC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReverseNetC, self).__init__()\n",
    "        # in channels, out channels, kernel size\n",
    "        self.conv0 = nn.Conv2d(1, 8, (1, 1))\n",
    "        self.activ0 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(8, 16, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ1 = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 8, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ2 = nn.PReLU()\n",
    "        self.conv3 = nn.Conv2d(8, 4, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "        self.activ3 = nn.PReLU()\n",
    "        self.conv4 = nn.Conv2d(4, 1, (3, 3), padding=(1, 1), padding_mode='circular')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activ0(self.conv0(x))\n",
    "        x = self.activ1(self.conv1(x))\n",
    "        x = self.activ2(self.conv2(x))\n",
    "        x = self.activ3(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Reverse net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseForwardNet(nn.Module):\n",
    "    def __init__(self, ForwardNet, ReverseNet):\n",
    "        super(ReverseForwardNet, self).__init__()\n",
    "        self.reverse_net = ReverseNet()\n",
    "        # freeze the weights of the forward net\n",
    "        self.forward_net = ForwardNet()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.reverse_net(x)\n",
    "        x = self.forward_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_VERSION = 'C'\n",
    "\n",
    "if MODEL_VERSION == 'A':\n",
    "    ReverseNet = ReverseNetA\n",
    "elif MODEL_VERSION == 'B':\n",
    "    ReverseNet = ReverseNetB\n",
    "elif MODEL_VERSION == 'C':\n",
    "    ReverseNet = ReverseNetC\n",
    "\n",
    "rf_net = ReverseForwardNet(RelaxedForwardNet, ReverseNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf = Variable(torch.tensor(stop_boards).view(N, 1, 25, 25).float(), requires_grad=True)\n",
    "y_rf = Variable(torch.tensor(stop_boards).view(N, 1, 25, 25).float())\n",
    "X_valid_rf = y_valid\n",
    "y_valid_rf = y_valid\n",
    "\n",
    "rf_model_path = \"../models/johnson/reverse_forward.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relaxed_forward_net.load_state_dict(torch.load(relaxed_forward_model_path))\n",
    "# rf_net.load_state_dict(torch.load(rf_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_relaxed = Variable(torch.tensor(relaxed_start_boards).view(N, 1, 25, 25).float(), requires_grad=True)\n",
    "X_valid_relaxed = Variable(torch.tensor(relaxed_valid_start_boards).view(N_valid, 1, 25, 25).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-f968343f4012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     train(relaxed_forward_net, X_relaxed, y_relaxed, X_valid_relaxed, y_valid_relaxed, \n\u001b[0;32m---> 13\u001b[0;31m           optim.Adam, criterion, relaxed_forward_model_path, batch_size=128, num_epochs=3)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Load reverse-forward net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-04c5a396f782>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, y, X_valid, y_valid, optim, criterion, output_path, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/cs229/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/cs229/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Evolved the relaxed boards\n",
    "    y_relaxed = (actual_forward((X_relaxed > 0.5).int()) > 0.5).float()\n",
    "    y_valid_relaxed = (actual_forward((X_valid_relaxed > 0.5).int()) > 0.5).float()\n",
    "    \n",
    "    # Load relaxed forward net\n",
    "    relaxed_forward_net = RelaxedForwardNet()\n",
    "    relaxed_forward_net.load_state_dict(torch.load(relaxed_forward_model_path))\n",
    "    # Train relaxed forward net\n",
    "    criterion = nn.BCELoss()\n",
    "    train(relaxed_forward_net, X_relaxed, y_relaxed, X_valid_relaxed, y_valid_relaxed, \n",
    "          optim.Adam, criterion, relaxed_forward_model_path, batch_size=128, num_epochs=3)\n",
    "    \n",
    "    # Load reverse-forward net\n",
    "    rf_net = ReverseForwardNet(RelaxedForwardNet, ReverseNet)\n",
    "    rf_net.load_state_dict(torch.load(rf_model_path))\n",
    "    # Load relaxed forward net weights and freeze them\n",
    "    rf_net.forward_net.load_state_dict(torch.load(relaxed_forward_model_path.format(i)))\n",
    "    for param in rf_net.forward_net.parameters():\n",
    "        param.requires_grad = False\n",
    "    rf_net.cuda()\n",
    "        \n",
    "    # Train reverse net\n",
    "    criterion = nn.BCELoss()\n",
    "    train(rf_net, X_rf, y_rf, X_valid_rf, y_valid_rf, \n",
    "          optim.Adam, criterion, rf_model_path, batch_size=128, num_epochs=3)\n",
    "    \n",
    "    # Create new X_relaxed and y_relaxed\n",
    "    X_relaxed = rf_net.reverse_net.cpu()(y_relaxed)\n",
    "    X_valid_relaxed = rf_net.reverse_net.cpu()(y_valid_relaxed)\n",
    "    \n",
    "    relaxed_forward_net.load_state_dict(torch.load(relaxed_forward_model_path.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data MAE\n",
    "train_mae = get_forward_mae(rf_net, rf_model_path, stop_boards, stop_boards, N)\n",
    "print(\"The training data MAE is {:.6f}.\".format(train_mae))\n",
    "\n",
    "# Test data MAE\n",
    "test_mae = get_forward_mae(rf_net, rf_model_path, test_stop_boards, test_stop_boards, N_test)\n",
    "print(\"The test data MAE is {:.6f}.\".format(test_mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
